# Data

## Relational Databases

* Concepts: Pre-Read

[RDBMS Concepts](https://beginnersbook.com/2015/04/rdbms-concepts/)

* Tables
* Record/Tuple
* Field/Column
* Domain (permitted values)
* Instance
* Keys

### Key Concepts

* Relational DB's are used for data storage, with a specific set of goals: Efficiency, Ease of Access, Organiation, Handle Relations.
* ACID - atomicity, consistency, isolation, durability.
* Atomicity - ensures that partially failed transactions can be rolled back.
* Consistency - ensures that the balance between transactions does not even occur.
* Isolation guaragtees that multiple sub-operations which make up a transaction do not overwrite each other.
* Durability ensures that changes are applied correctly during recovery.

CRUD Operations

* CRUD - create, read, update, delete.
#### Classifying Operation by Type:

Management Operations

* DDL - definition
* DML - manipulation
* DCL - control
* TCL - transactions

* insert - transaction; inserting a value is a transaction
* create - definition; creating a table is to define something, no values inserted
* drop - definition; dropping or deleting tables
* delete - manipulation; deleting values is equivalent to changing, manipulating
* update - manipulation; updating a value
* commit - transaction; creating an entry
* rollback - manipulation; similar to updating
* truncate - manipulation; similar to deleting
* alter - manipulation; similar to deleting
* grant - control; granting actual outside access
* revoke - control; granting or revoking access

#### Other Definitions

* Constraints - rules for data that can be stored. Queries fail if defines rules are violated.
* Indexes - using a B+ structure, speeding up the fetching of rows and queries.
* Joins - allows you to fetch related data from multiple tables
* Access control - privledged accounts for admin tasks and regular accounts for clients

#### Popular Databases

* Commercial, closed source - Oracle, Microsoft SQL Server, IBM DB2
* Open source with optional paid support - MySQL, MariaDB, PostgreSQL

### MySQL

* Read through this...similar to Postgres

* Has an application layer which covers connection handling, authentication, security.
* Has a server layer which covers backup and restore, SQL interface, parser and optimization.
* Storage engine - InnoDB, which is the most widely used, transaction support, ACID compliant, crash recovery, etc.
* Possible to migrate from one storage engine to another, but this migration locks tables for all operations.

### InnoDB

* Memory - buffer pool, LRU cache of frequently used data. Has a change buffer, adaptive hash index, log buffer, which holds log data before flush to disk.
* Disk - stores data in rows and columns.
* Indexes - helps find rows with specific column values quickly.
* Redo logs - all transactions are written to them.

### Backup and Recovery

* Physical Backup - data directory as it is on the disk
* Logical Backup - table structure and records in it.

#### Mysqldump

This utility is available with MySQL installation, helps with the logical backup of the database, outputs a set of SQL statements to reconstruct the data, it is not recommended to use for large tables as it may take a lot of time and the file size will be huge.

```
mysqldump [options] > dump_output.sql
```

To dump all databases...

```
mysqldump -u<user> -p<pwd> --all-databases > dbs.sql
```
To dump a single database, a specific one:

```
mysqldump -u<user> -p<pwd> --databases db1 db2 > dbs.sql
```
Specific tables:

```
mysqldump -u<user> -p<pwd> db1 table1 table2 > db1_tables.sql
```
##### Restoring

From shell

```
mysql -u<user> -p<pwd> < all_dbs.sql
```

from shell if database already created:

```
mysql -u<user> -p<pwd> db1 < db1.sql
```

or from within MySQL Shell:

```
mysql> source all_dbs.sql
```

##### Percona Xtrabackkup

Utility which provides backups.

* Full, partial or cumulative.

* There are other SQL backup tools. - point-in-time recoveries, mysqlpump, mysqldumper, etc.

### MySQL Replication

Replication enables data from on MySQL host (primary) to be copied to another host, (replica). MySQL replication is asynchronous by nature by default, but it can be changed to semi-synchronous.

#### Applications 

* Read-scaling: set up as many replicas as we need and scale reads through them, which improves write performance, as the primary source is only dedicated to writes and not reads.
* Backups using replicas.
* Disaster recovery.

#### Asynchronizations

* Asynchronous is the default method. One host serves as the primary and one or more as the replicas.
* Semi-synchronous means that a commit is performed on the primary host is blocked until at least one of the replicas acknowledge it. This ensures strong consistency.
* Delayed - we can deliberaly delay, so that the replicas get updated and show the source of truth later. This safeguards against human errors, e.g. if a drop database is executed by mistake on the primary, the replica may still have the original copy, for a while at least.
### Operational Concepts

#### Select Query, Joins, 

* SELECT is the most commonly used MySQL command.
* Full SQL queries can be built using SELECT and additional suffix terms to a search string.

I have personally gone through many SQL Query exercises and [posted them here](https://github.com/pwdel/codepractice/tree/main/HackerRank/HR-SQL), and have used SQL Extensively through a python flask ORM tool called SQLAlchemy.

* Joins are simply used to join tables, this is something I have used extensively with SQLAlchemy.
#### Query Performance

* Crucial aspect of relational databases.
* The important task is to identify slow queries and try to improve performance by either re-writing them or creating proper indexes on the tables involved.

##### Slow Query Log

* Contains SQL Statements that take a longer time to execute than the config parameter, long_query_time.  These queries are candidates for optimization.
* There are some utilities that summarize slow query logs, such as mysqldumpsslow, pt-query-digest (Percona).

##### EXPLAIN

The explain command is used with any query to analyze. Key aspects to the output:

* Partitions
* Possible_keys
* key
* Rows
* Filtered
* Extra - how MySQL evaluates

For a given query, you can determine and improve performance based upon the output of Explain.

##### Indexes

* Indexes are used to speed up selecting relevant rows for a given column value.
* Without an index, MySQL starts with the first row and goes through the entire table to find matching rows, so the operation can become costly.
* Primary key is an index which is the fastest and is stored along with the table data. 

```
create index idx_salary on salaries(salary)
```

* This will put an idx_salary under, "possible_keys" for relevant frequent queries, and nothing under non-relevant queries. This brings the query down to a negligible amount, rather than closer to 1 second.
* Cascading keys can be made to further bring the query need downward.

Basically these indexes help with optimization.

### Lab

* Has the user run a mysql instance and run queries.
* Has user view tables, view all indexes on a table, use EXPLAIN
* Identifying slow queries with, "EXPLAIN."
### Conclusion

Further reading:

* Normalization
* Routines, Triggers
* Views
* Transaction Isolation Levels
* Sharding
* Monitoring, Backups

## NoSQL

* NoSQL just means, "non-relational database."  It's a generalized term.
* Putting developer hours first rather than storage costs.

### NoSQL Types

1. Document Databases - store data in documents similar to JSON. An example would be MongoDB's bson format.

2. Key-Value Databases - similar to the document concept, but with keys and values. Good for large amounts of data but don't want to perform complex queries to retrieve it. Example is Redis or DynamoDB.

3. Wide-Column Stores - store data in tables, rows and dynamic columns. Provides flexibility over relational databases, because each row is not required to have the same columns, e.g. you can map multiple rows and columns across, "sheets / tables." These might also be considered to be key-value databases in 2-dimensions. This may be used to store large amounts of data and you can predict what query patterns might be like. Cassandra and HBase are two of the most popular data stores.

4. Graph Databases - Store data in nodes and edges.  Nodes might store information about people, places, things and edges store information about relationships between the nodes. Some of these databases may depend upon a relational engine which stores the graph data in a table, which imposes another level of abstraction between the graph database and how it is actually stored. Others may use key-value store or document-oriented database for the actual storage. These databases excel in areas where you need to traverse relationships and look for patterns such as social networks, fraud detection, recommendation engines, etc. Neo4j is an example of a graph database.

#### Performance, Scalability, Flexibility, Complexity, Functionality

* Performance, Scalability, Flexibility, Complexity, Functionality are different considerations which may be taken into effect when selecting a database type. Different NoSQL types listed above have different ratings on these parameters, for example, Key Value has high performance and scalability, flexibility, but might not have as much functionality as Graph databases, which may have lower performance and scalability, and high complexity.

* Most NoSQL databases do not support multi-record ACID transactions, however some, such as MongoDB do.
* Joins are not really required in noSQL databases.

### CAP Theorem

What is the real purpose of NoSQL databases?

* Consistency

- The system should be available after an execution. When writes to a shared source are available to all readers, the ratio of time availability is the consistency.

* Availability

- How much a system respons to the loss of functionality of different hardware or software systems. High availability implies the system is still available to handle operations, even if a certain part of the system is down due to failure or upgrade.

* Partition Tolerance

- The capability for a system to continue operations in the event of a network partition. Network partitions occur when a failure causes two or more islands of networks where systems cannot talk to each other across the islands temporarily.

Allegedly, two of these three characterisitcs can be chosen in a shared-data system. "Consistency, Availability, Partition Tolerance - Pick Two."  Systems which value reliability may 

#### Eventual Consistency

* Eventual consistency means that all readers will see writes as time goes on.
* Clients face inconsistent states of data as updates are in progress.

NoSQL systems support different levels of eventual consistency, for example:

* Read Your Own Writes Consistency - Clients see updates immediately after they are written. Reads can hit nodes other than the one where it was written.

* Session Consistency - Clients will see the updates to their data within a session scope. This generally indicates that reads and writes occur on the same server.

* Casual Consistency - Write operations are related by potential causality are seen by each process of the system in order. Different processes may observe concurrent writes in different orders.

The consistency model chosen determines where requests are routed.

##### CAP (Consistency, Availability, Partition Tolerance) Alternatives

* Consistency + Availability ... 2-phase commits, cache invalidation. For example, xFS file system, single-site databases.

* Consistency + Partition Tolerance ... pessemistic locking, make minority partitions unavailable.

* Availability + Partition Tolerance ... expirations / leases, for example, DNS, Web Caching.


Thinking about all of the problems inherent in DNS, the fact that it seems to bring important services across the web down on a consistent basis, has to do with the CAP Theorem, in which the feature necesity of Availbility and Partition Tolerance sacrifices Consistency, or rather brings about Casual Consistency, basically allowing different systems to observe writes regardless of whether everything has been organized consistently in a cache, basically Availability and Partition Tolerance taking precedence over the ability to see everything consistently across the web. 

#### Versioning of Data in Distributed Systems

* Timestamps - sort updates based upon chronological order. Gets more complicated as geography comes into play.

* Optimistic Locking - Associate a unique value like a clock or counter with every update. When a client wants to update data, it has to specify which version of data needs to be updated, which means data versions are kept track.

* Vector Clocks - Tuple of clock values from each node. Clock values may be real timestamps or not.

Vector clocks have no dependency on synchronization, and have no total ordering necessary, just has different nodes.

#### Partitioning

* When the amount of data crosses the capacity of a single node, it has to be split, creating replicas for load balancing and disaster recovery. Depending upon how dynamic the infrastructure is, we have different approaches:

##### Memory Cached

* Partitioned in-memory databases that are primarily used for transient data. Used as a front for traditional RDBMS. More frequently used data is replicated from a RDBMS to a memory database to facilitate fast queries.

Note - there was a hacker news article with someone who had worked at Google reminicisng about the, "I can't even count that low," problem. Google in the early 2010's dealt with databases that were so large and geographically distributed that storing them in memory was not an option, hence, something like a Memory Cached Database may have been used.
##### Clustering

* Clients don't need to know where the actual data is residing and which node it's talking to. This is very common in traditional RDBMS, where it can help in scaling the persistent layer.
##### Seperating Reads from Writes

* Incoming writes are sent to a single node (Leader) and the rest of the replicas (Followers) handle read requests.
* Leader replicates writes asynchronously to all followers, however the write lag can't be completely avoided.
* Sometimes leaders can crash before replicating all of the data to a follower. When this happens, a follower with the most consistent data can be turned into a leader.
* Write lag cannot be completely avoided, there has to be some kind of leader-follower catch up logic.
##### Sharding

* Dividing data in such a way that data is distributed evenly, both in terms of storage and processing power.
* Similar and related data can be stored together to facilitate faster access.
* Shard can in turn be further replicated to meet load balancing or disaster recovery requirements.
* Reads can be distributed across multiple replicas.

Basically, put the relevant data in the area of higher processing power, if it is used more.

### Hashing

* Map one piece of data, an object description, of arbitrary size to another piece of data, known as a hash code, or simply, hash.
* It's important to consistently map a key to a server/replica.

Example:

* Use a modulo function:

```
_p = k mod n_
```

where:

```
p -> partition,


k -> primary key


n -> no of nodes
```
The downside is, whenever the cluster topology changes, the data distribution also changes. When you are dealing with memory caches, it will be easy to distribute partitions around. Partitions can reorder themselves upon nodes joining a topology, as they are not necessarily duplicates of one another, but rather holding different data.

### Consistent Hashing

* Distributed hashing scheme which operates independently of the number of servers or objects in a distributed hash table by assigning them a position on an abstract table, or a hash ring.
* If the hash function h() generates a 32-bit integer, the server number can be determined with a key, k, and find a server h(s) which has the smallest integer which is larger than h(k). The table is assumed to be circular, so it can be rotated.

### Quorum

* Quorum refers to the minimum number of nodes which must be online and able to communicate with each other, to prevent the cluster from stopping running. Commonly it's N/2+1 where N is the number of nodes. In a 3 node cluster, you need 2 nodes running, etc.

Network problems can cause communication failure among cluster nodes. One set of nodes might be able to communicate together across a functioning part of the network but not able to communicate with a different set of nodes in another part of the network.



## Big Data

### Intro

* Definition of Big Data and where it is a good fit.

### Overview

* Large data assets that cannot be produced using traditional computing techniques, it's a complete subject requiring different tools, techniques and frameworks.
* Structured data, unstructured data, semi-structured data.
* Volume, Variety Velocity, Variability
* Stock Exchanges, Social Media Sites, Jet Engines, etc.

### Usage of Big Data Techniques

Example - Traffic Lights

* There are 300,000 traffic lights in the US.
* Place a device on each of them to collect metrics and send to central metrics collection system.
* If each device sends 10 events per minute, you have 432^7 events per day.
* How would you go about processing how many of the signals are green at a specific time on a specific day?

Example - Unified Payments Interface Transactions

* 1.15 billion UPI Transactions in the month of October in India.
* Extrapolate this data to a year, to find out common payments that were happening through a particular UPI ID, how would you go about that?

### Evolution and Architecture of Hadoop

* Built for Commodity Hardware
* Originally designed for on-prem and cloud hybrid solutions
* Map-Reduce introduced later, Map is a way of splitting data while Reduce is a way of counting by category and delivering a final result query.

#### Other Toolsets for Hadoop

* Hive - SQL Like Query Toolset
* Pig - Uses scripting language, Pig Latin, which is workflow driven, don't need to be Java expert.
* Spark - provides primitives for in-memory cluster computing that llows user programs to load data into a cluster's memory and query it repeatedly.
* Presto - high performance, distributed SQL for Big Data.

### Data Serialization and Storage

* Transports the data over the network, or store on some persistent storage, we use the process of translating data structures objects into binary or textual form.
* Avro data is stored in a container file .avro, stored in the data file.
* Apache Hive provides support to store a table as Avro can query  data in serialization format.